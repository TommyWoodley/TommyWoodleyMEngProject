
@article{greenwade93,
    author  = "George D. Greenwade",
    title   = "The {C}omprehensive {T}ex {A}rchive {N}etwork ({CTAN})",
    year    = "1993",
    journal = "TUGBoat",
    volume  = "14",
    number  = "3",
    pages   = "342--351"
}
@inproceedings{learnedTetheredPerchingFabian,
title = "Learning Tethered Perching for Aerial Robots",
abstract = "Aerial robots have a wide range of applications, such as collecting data in hard-to-reach areas. This requires the longest possible operation time. However, because currently available commercial batteries have limited specific energy of roughly 300 W h kg-1, a drone's flight time is a bottleneck for sustainable long-term data collection. Inspired by birds in nature, a possible approach to tackle this challenge is to perch drones on trees, and environmental or man-made structures, to save energy whilst in operation. In this paper, we propose an algorithm to automatically generate trajectories for a drone to perch on a tree branch, using the proposed tethered perching mechanism with a pendulum-like structure. This enables a drone to perform an energy-optimised, controlled 180° flip to safely disarm upside down. To fine-tune a set of reachable trajectories, a soft actor critic-based reinforcement algorithm is used. Our experimental results show the feasibility of the set of trajectories with successful perching. Our findings demonstrate that the proposed approach enables energy-efficient landing for long-term data collection tasks.",
author = "Fabian Hauf and Kocer, {Basaran Bahadir} and Alan Slatter and Nguyen, {Hai Nguyen} and Oscar Pang and Ronald Clark and Edward Johns and Mirko Kovac",
note = "Funding Information: We also thank former Aerial Robotics Lab members who have explored the work at various levels, particularly Bojia Mao and Kobi Kelemen. This work was partially supported by funding from EPSRC (award no. EP/N018494/1, EP/R026173/1, EP/R009953/1, EP/S031464/1, EP/W001136/1), NERC (award no. NE/R012229/1) and the EU H2020 AeroTwin project (grant ID 810321). Mirko Kovac is supported by the Royal Society Wolfson fellowship (RSWF/R1/18003). For the purpose of open access, the author(s) has applied a Creative Commons Attribution (CC BY) license to any Accepted Manuscript version arising. Publisher Copyright: {\textcopyright} 2023 IEEE.; 2023 IEEE International Conference on Robotics and Automation, ICRA 2023 ; Conference date: 29-05-2023 Through 02-06-2023",
year = "2023",
doi = "10.1109/ICRA48891.2023.10161135",
language = "English",
series = "Proceedings - IEEE International Conference on Robotics and Automation",
publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
pages = "1298--1304",
booktitle = "Proceedings - ICRA 2023",
address = "United States",
}
@incollection{abbeelRLAerobaticFlight,
    author = {Abbeel, Pieter and Coates, Adam and Quigley, Morgan and Ng, Andrew Y.},
    isbn = {9780262256919},
    title = "{An Application of Reinforcement Learning to Aerobatic Helicopter Flight}",
    booktitle = "{Advances in Neural Information Processing Systems 19: Proceedings of the 2006 Conference}",
    publisher = {The MIT Press},
    year = {2007},
    month = {09},
    doi = {10.7551/mitpress/7503.003.0006},
    url = {https://doi.org/10.7551/mitpress/7503.003.0006},
    eprint = {https://direct.mit.edu/book/chapter-pdf/2139612/9780262256919\_caa.pdf},
}
@misc{deepQLearningFromDemo,
      title={Deep Q-learning from Demonstrations}, 
      author={Todd Hester and Matej Vecerik and Olivier Pietquin and Marc Lanctot and Tom Schaul and Bilal Piot and Dan Horgan and John Quan and Andrew Sendonaris and Gabriel Dulac-Arnold and Ian Osband and John Agapiou and Joel Z. Leibo and Audrunas Gruslys},
      year={2017},
      eprint={1704.03732},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@article{SACfDMaplessNavigation,
abstract = {This paper is concerned with the problems of mapless navigation for unmanned aerial vehicles in the scenarios with limited sensor accuracy and computing capability. A novel learning-based algorithm called soft actor-critic from demonstrations (SACfD) is proposed, integrating reinforcement learning with imitation learning. Specifically, the maximum entropy reinforcement learning framework is introduced to enhance the exploration capability of the algorithm, upon which the paper explores a way to sufficiently leverage demonstration data to significantly accelerate the convergence rate while improving policy performance reliably. Further, the proposed algorithm enables an implementation of mapless navigation for unmanned aerial vehicles and experimental results show that it outperforms the existing algorithms.},
author = {Yang, JiaNan and Lu, ShengAo and Han, MingHao and Li, YunPeng and Ma, YuTing and Lin, ZeFeng and Li, HaoWei},
address = {Beijing},
copyright = {Science China Press 2023},
issn = {1674-7321},
journal = {Science China. Technological sciences},
keywords = {Algorithms ; Engineering ; Machine learning ; Maximum entropy ; Navigation ; Unmanned aerial vehicles},
language = {eng},
number = {5},
pages = {1263-1270},
publisher = {Science China Press},
title = {Mapless navigation for UAVs via reinforcement learning from demonstrations},
volume = {66},
year = {2023}}

@article{aerialNavReview,
author = {AlMahamid, Fadi and Grolinger, Katarina},
issn = {0952-1976},
journal = {Engineering applications of artificial intelligence},
language = {eng},
pages = {105321-},
title = {Autonomous Unmanned Aerial Vehicle navigation using Reinforcement Learning: A systematic review},
volume = {115},
year = {2022},
}

@article{droneReview,
    author = {Chan, K. W. and Nirmal, U. and Cheaw, W. G.},
    title = "{Progress on drone technology and their applications: A comprehensive review}",
    journal = {AIP Conference Proceedings},
    volume = {2030},
    number = {1},
    pages = {020308},
    year = {2018},
    month = {11},
    abstract = "{The current work is a development of an interface to control a wireless drone for home security applications. A review had been done for the categorization of drones and drones’ trend for recent years. It was found that drones were basically categorized by weight and flight range and number of drones increased by leaps and bounds from year 2003 to year 2017. The history of drones and recent-year-founded applications were being reviewed as well. A research was being done on 19 types of drones which are available in current market such as DJI Phantom 4 and GoPro Karma and their applications. To the summary of the review, drones’ technology will be advancing with the current rate of development of technology and it will be more applicable to our daily life in the future. Proposed future research pathways on drone’s technology and their applications were being done as well to understand the future trend and possible advancement for drones.}",
    issn = {0094-243X},
    doi = {10.1063/1.5066949},
    url = {https://doi.org/10.1063/1.5066949},
    eprint = {https://pubs.aip.org/aip/acp/article-pdf/doi/10.1063/1.5066949/14170335/020308\_1\_online.pdf},
}
@INPROCEEDINGS{environmentalSensing,
  author={Kocer, Basaran Bahadir and Ho, Boon and Zhu, Xuanhao and Zheng, Peter and Farinha, André and Xiao, Feng and Stephens, Brett and Wiesemüller, Fabian and Orr, Lachlan and Kovac, Mirko},
  booktitle={2021 Aerial Robotic Systems Physically Interacting with the Environment (AIRPHARO)}, 
  title={Forest Drones for Environmental Sensing and Nature Conservation}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/AIRPHARO52252.2021.9571033}}

@Article{droneBattery,
AUTHOR = {Hwang, Myeong-hwan and Cha, Hyun-Rok and Jung, Sung Yong},
TITLE = {Practical Endurance Estimation for Minimizing Energy Consumption of Multirotor Unmanned Aerial Vehicles},
JOURNAL = {Energies},
VOLUME = {11},
YEAR = {2018},
NUMBER = {9},
ARTICLE-NUMBER = {2221},
URL = {https://www.mdpi.com/1996-1073/11/9/2221},
ISSN = {1996-1073},
ABSTRACT = {The practically applicable endurance estimation method for multirotor unmanned aerial vehicles (UAVs) using a battery as a power source is proposed. The method considers both hovering and steady-level flights. The endurance, thrust, efficiency, and battery discharge are determined with generally available data from the manufacturer. The effects of the drag coefficient related to vehicle shape and payload weight are examined at various forward flight speeds. As the drag coefficient increases, the optimum speed at the minimum required power and the maximum endurance are reduced. However, the payload weight causes an opposite effect, and the optimal flying speed increases with an increase in the payload weight. For more practical applications for common users, the value of S &times; Cd is determined from a preliminary flight test. Given this value, the endurance is numerically estimated and validated with the measured flight time. The proposed method can successfully estimate the flight time with an average error of 2.3%. This method would be useful for designers who plan various missions and select UAVs.},
DOI = {10.3390/en11092221}
}
@article{droneSunlight,
abstract = {Sunlight energy is potentially excellent for small drones, which can often operate during daylight hours and fly high enough to avoid cloud blockade. However, the best solar cells provide limited power, compared to conventional power sources, making their use for aerial vehicles difficult to realize, especially in rotorcraft where significant lift ordinarily generated by a wing is already sacrificed for the ability to hover. In recent years, advances in materials (use of carbon‐fiber components, improvement in specific solar cells and motors) have finally brought solar rotorcraft within reach. Here, the application is explored through a concise mathematical model of solar rotorcraft based on the limits of solar power generation and motor power consumption. Multiple solar quadcopters based on this model with majority solar power are described. One of them has achieved an outdoor airtime over 3 hours, 48 times longer than it can last on just battery alone with the solar cells carried as dead weight and representing a significant prolongation of drone operation. Solar‐power fluctuations during long flight and their interaction with power requirements are experimentally characterized. The general conclusion is that solar cells have reached high enough efficiencies and can outperform batteries under the right conditions for quadcopters.
A quadcopter using on‐board solar module to harvest sunlight outdoors achieves over 3 h airtime, the longest for quadcopters without using chemical fuels and about 48 times more than using battery only. The accumulated charge generated from sunlight is 33 300 mAh, which would require a battery much heavier than the solar module and beyond the thrust capability to carry.},
author = {Lin, Ching‐Fuh and Lin, Ta‐Jung and Liao, Wei‐Sheng and Lan, Hsiang and Lin, Jiun‐Yu and Chiu, Chi‐Han and Danner, Aaron},
address = {Weinheim},
copyright = {2020 The Authors. Published by Wiley‐VCH GmbH},
issn = {2198-3844},
journal = {Advanced science},
keywords = {Aircraft ; airtime ; Alternative energy sources ; Automotive engineering ; Aviation ; chemical sciences ; Computer science ; Daylight ; Dead weight ; Drone ; Efficiency ; engineering and technology ; Fossil fuels ; general chemistry ; Motor power ; nano-technology ; nanoscience & nanotechnology ; natural sciences ; Power-to-weight ratio ; Quadcopter ; quadcopters ; Science ; solar cells ; Solar energy ; Solar power ; Sunlight ; Vehicles},
language = {eng},
number = {20},
pages = {2001497-n/a},
publisher = {John Wiley & Sons, Inc},
title = {Solar Power Can Substantially Prolong Maximum Achievable Airtime of Quadcopter Drones},
volume = {7},
year = {2020}}

@article{rlIntroSuttonBarlo,
author = {Johnson, Jeffrey D and Li, Jinghong and Chen, Zengshi},
copyright = {2000 Elsevier Science Ltd},
issn = {0925-2312},
journal = {Neurocomputing (Amsterdam)},
language = {eng},
number = {1},
pages = {205-206},
publisher = {Elsevier B.V},
title = {Reinforcement Learning: An Introduction: R.S. Sutton, A.G. Barto, MIT Press, Cambridge, MA 1998, 322 pp. ISBN 0-262-19398-1},
volume = {35},
year = {2000},
}

@article{humanLevelControlDQN,
abstract = {The theory of reinforcement learning provides a normative account (1), deeply rooted in psychological (2) and neuroscientific (3) perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems (4,5), the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms (3). While reinforcement learning agents have achieved some successes in a variety of domains (6-8), their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks (9-11) to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games (12). We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
author = {Volodymyr Mnih and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
address = {London},
copyright = {COPYRIGHT 2015 Nature Publishing Group},
issn = {0028-0836},
journal = {Nature (London)},
keywords = {Algorithms ; Artificial intelligence ; Artificial neural network ; Computer & video games ; Computer science ; General video game playing ; Methods ; Neural networks ; Physiological aspects ; Psychological aspects ; Q-learning ; Reinforcement ; Reinforcement learning ; Reinforcement learning (Machine learning) ; Sensory processing ; Set (psychology) ; Temporal difference learning},
language = {eng},
number = {7540},
pages = {529-533},
publisher = {Nature Publishing Group},
title = {Human-level control through deep reinforcement learning},
volume = {518},
year = {2015},
}

@INPROCEEDINGS{fyp12-waypoint-navigation,
  author={Eslamiat, Hossein and Li, Yilan and Wang, Ningshan and Sanyal, Amit K. and Qiu, Qinru},
  booktitle={2019 18th European Control Conference (ECC)}, 
  title={Autonomous Waypoint Planning, Optimal Trajectory Generation and Nonlinear Tracking Control for Multi-rotor UAVs}, 
  year={2019},
  volume={},
  number={},
  pages={2695-2700},
  doi={10.23919/ECC.2019.8795855}}
@inproceedings{fyp12-waypoint-nav2,
  title={Autonomous waypoints planning and trajectory generation for multi-rotor UAVs},
  author={Li, Yilan and Eslamiat, Hossein and Wang, Ningshan and Zhao, Ziyi and Sanyal, Amit K and Qiu, Qinru},
  booktitle={Proceedings of the Workshop on Design Automation for CPS and IoT},
  pages={31--40},
  year={2019}
}
@misc{fyp14-rl-imperfect-demos,
      title={Reinforcement Learning from Imperfect Demonstrations}, 
      author={Yang Gao and Huazhe Xu and Ji Lin and Fisher Yu and Sergey Levine and Trevor Darrell},
      year={2019},
      eprint={1802.05313},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}