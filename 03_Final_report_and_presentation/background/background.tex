\chapter{Background}
% 10-20 pages
% This should form the bulk of the interim report. 
% You should consider that your objective here is to produce a near final version of the background section, as it will appear in your final report. 
% All of this material should be re-usable, so it is worth getting it right at this stage of the project.  
% The details of what to include can be found in the Project Report guidelines.

% - Perching Approaches
% - Reinforcment Learning
%   - Traditional
%     - Previous Work - done
%     - FYP8 - Human-Level Control through Deep Reinforcment Learning
%   - Apprentiship Reinformcent Learning 
%     - FYP7 - "An Application of Reinformcent Learning to Aerobatic Helicopter Flight (P. Abel 2007)" - done
%   - Demonstration
%     - FYP9 - DeepQ Learning from Demonstration
%     - FYP-16 - Forgetful Experience Replay in Hierechical Reinfrocment Learning from Expert Demonstrations.
%     - FYP17 - Mapless navigation for UAVs via Reinforcement Learning
%   - Transfer Learning
%     - FYP10 - Soft Actor-Critic with Inhibitory Networks for Retraining UAV Controllers Faster
%   - Learned Skills
%     - FYP-15 - Demonstration Guided Reinforcment Learning with Learned Skills

% Previous Work - Fabian

\section{Perching Approaches}

\section{Traditional Reinforcement Learning}

Previous work has explored the use of Reinforcement Learning in automatically generating trajectories for drone perching\cite{learnedTetheredPerchingFabian}.
In this approach a Soft Actor Critic algorithm was employed to develop a series of energy optimised trajectories.
The trajectory was divided into three seperate stages.
The initial approach and contact phases used trajectories formulated using analytical solutions.
The Reinformcent Learning aspect was specifically applied for the more complex manuver of flipping the drone beneath the branch.

\todo{diagram for this manuever}
A Markov Decision Process was defined using observations $o_{t} \in O$ where $o_{t}$ denotes the drone's relative position and $a_{t}$ determines its roll rate.
An experimental baseline provided a potential solution using a constant roll rate.
This method integreated two components of a reward function.
In the initial training stages, the algorithm priotitised conforming to the baseline.
As training progresses, there is a shift in the reward function's focus, increasingly favouring a faster, more energy efficient trajectory.
This shift allows a greater exploration of the parameter space.

\[
R_{1}(s_{t}, a_{t}) = 
\begin{cases} 
I(s_{t}, a_{t}) & \text{for } t \leq t_{I} \\
I(s_{t}, a_{t}) M(t) & \text{else} 
\end{cases}
\]

\[I(s_{t}, a_{t}) = 3 \times 10 ^ 5 \times (0.1 - min(|s_{\alpha} - s'_{\alpha}|, 0.1)) ^ 4\]
\[M(t) = \frac{t_{max} - t}{t_{max} - t_{I}}\]

\[
R_{2}(s_{t}, a_{t}) =
\begin{cases}
  R_{L} + \frac{\Delta X_{target} - 0.005}{l_{r}} \times 50 & \text{for } \frac{d_{target}}{d_{drone}} > 1 \\
  R_{L} + \frac{\Delta X_{branch}}{d_{drone}} \times 100 & \text{else}
\end{cases}
\]

\todo{rephrase this section}
where $s_{\alpha}$ and $s_{\alpha}$ are the roll angel of the current and the baseline state; 
$t_{max}$ is the maximum time of the simulation; 
$t_{I}$ is the time where the final point should be reacted.
where RL is the reward from the last step dtarget is the distance to the target position; 
ddrone is the size of the drone; 
∆Xtarget and ∆Xbranch is the change in the distance made to the final position and to the branch, and lr is the length of the rope.

However this method may constrain the range of possible solutions.
By favouring approaches that resemble the baseline, this could prevent the system from learning novel, potentially more efficient strategies.
Since just roll rate was explored, this limits the range of solutions into 2 dimensions which reduces the computational complexity but may not fully explore paths.
Additioanlly one of the main challenges in the previous work was simulation.
Accurately modelling the dynamics between the tether and the drone presented significant challenges.
Performing training in a real-world setting would be exceptionally difficult, given the extensive number of trials required and the risk of errors causing physical damage to the drone.

There is a need to devise a system capable of efficiently learning from a very limited number of experiments, while ensuring safety of the drone. \\\\

\section{Reinforcement Learning with Demonstrations}

% FYP7 - "An Application of Reinformcent Learning to Aerobatic Helicopter Flight (P. Abel 2007)" - Demonstration
One of the earliest applications of Reinforcement Learning (RL) was in Aerobatic Helicopter Flight\cite{abbeelRLAerobaticFlight}.
Inverse Reinforcment Learning(IRL) was employed to learn a helicopter model and its associated reward function from human demonstrations.
This led to the first successfuly autonomous completion of four complex manuevers.
This apporach was motivated by the high amount of exploratory searching required in conventional Reinforcement Learning, which would lead to unsafe conditions and crashes.
Instead Apprentiship Learning was employed, allowing the system to create a model from demonstration flight data for each manuever.
This model was then trialled in simulation before finally being tried out on a physical helicopter.
The reward function consisted of 24 components, the balance between them was defined via an IRL algorithm.
However they found that strictly following the reward weights generated by this algorithm often led to unsafe conditions for the helicopter.
So, the reward weights were iteratively hand chosen from a mixture of the algorithm suggestions and inherent intuition. 
To address the risk of an unstable policy that was prone to fluctuating between extreme values, a term was added to the reward function, penalising change in inputs over successive time steps.

In the case of two manuevers, a single iteration of the process involving just 5 minutes of demonstration flight data proved sufficient to generate an accurate safe manuever.
For the remaining two manuevers, this process was repeated twice using further flight data in order to achieve a good result.
This study demonstraits that it is possible to effectively utilise demonstration data in order to massively speed up training of a reinforcement agent.
A key insight of this work was the minimisation of change in actions between subsequent steps to reduce the possibility of instability.
This particular work led to further exploration in the field of using demonstration data when trying to achieve a solution in areas where traditional simulation work was either too computationally expensive or difficult to achieve.

However a notable limitation of this approach is its reliance on expert generated demonstration data.
The demonstration data is presumed to represent the optimal solution, an assumption that is unlikely to hold true in the vast majority of applications.
The agent's performance is inherently limited by the expertise of the demonstrator.
Sicne the reward function is estabilished from the demonstration data, there lacks a mechanism to reward or even acknowlege improvements beyond the demonstrated capabilities.
This presents a challenge in scenerios where expert data is not necessarily possible or where surpassing the skill of a human pilot is required. \\\\

% - FYP9 - DeepQ Learning from Demonstration

More recent advancements in integrating demonstration data in training have utilised deep learning techniques to achieve significant success\cite{deepQLearningFromDemo}.
An algorithm, Deep Q-learning from Demonstrations(DQfD) was developed that makes use of a very small set of demonstration data to speed up the training time.
The performance of DQfD was evaluated using Atari Games, a popular benchmark in RL, DQfD outperformed its counterparts during the first million stages on 41 of 42 games, and acheieved state-of-the-art scores in 11 games.
This research was motivated by the unavailability of an accurate simulation environemnt.
DQfD incorporates a pre-training phase where the agent samples from demonstration datasets.
Multiple loss functions were employed, including supervised loss which was essential for the pre-training phase.
By its nature, demonstration data covers a very narrow margin of the state space, often exclusing potentially hazardous actions.
However simply using the demonstration data as part of an experience replay buffer is not sufficient as the agent will not adequently learn that these non-provided actions are more likely to be dangerous.
Without this supervised loss, the agent might favour these unseen areas of the state space which are likely to contain dangerous actions.
L2 regularisation loss was applied to the weights and biases of the network to ensure that the agent doesn't overfit consdiering the very small number of demonstration examples.
By incorporating these elements, DQfD accelerates the training process.

Once pre-training is complete, the system begins interacting with the actual environment, collecting additional data in a manner similar to standard DQN learning.
These additional experiences are collected in a replay buffer along with the original demonstration examples.
The demonstrated data is kept always kept in the buffer and not overridden, additionally it is sampled at a higher frequency than the agent collected data.
This effectively treats the demonstrated data a special type of collected data.
This provides a balance between the imitation and further exploration of the state space to produce "superhuman" results.
The DQfD agent additonally has a strong resiliance to suboptimal demonstrations or outliers.
The agent outperformed the worst demonstration provided in 29 out of 42 games.
In 14 of the 42 games, the agent was able to surpass the performance of the best demonstration provided, further proving the possibility for improvement beyond the provided examples.

This research shows the potential ability to overcome some of the difficulties as encoutered in the drone perching environemnt where the dynamics of the environment are complex to simulate.
This study demonstraits that utilising demonstration data can be a valuable way to address the safety aspects encountered when using real-world environments.
This is achieved with a relatively small amount of demonstration data from several minutes of gameplay per game.
However it is unknown whether demonstration data alone would provide enough safety for the drone trajectories.
Additionally increasing the performance from the demonstration data still requires a high number of training episodes.
Although this is certianly an improvement, it would likely still take a very large amount of training time to sucessfully improve which is key to becoming better than the given demonstrations and allowing a drone to develop good trajectories from non-expert demonstrations.
Finally this research only covers domains with discrete action spaces, in the drone perching environment, we have the ability to perform actions across a continuous action space.
This is likely to be more computationally expensive which is another challenge that will need to be overcome.
One of the major differences is the reward associated.
In the Atari games environment, the reward is tied directly to the score achieved by the agent, this makes evaluating performance clear.
However in the drone perching environment this is more challenging.
It is not immediately clear how to balance rewards between actually achieving the perching action, with safety to prevent crashes and speed and energy efficiency.
This makes evaluating the performance less clear.