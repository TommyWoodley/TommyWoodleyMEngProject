\chapter{Background}
% 10-20 pages
% This should form the bulk of the interim report. 
% You should consider that your objective here is to produce a near final version of the background section, as it will appear in your final report. 
% All of this material should be re-usable, so it is worth getting it right at this stage of the project.  
% The details of what to include can be found in the Project Report guidelines.

% - Perching Approaches
% - Reinforcment Learning
%   - Traditional
%     - Previous Work - done
%     - FYP8 - Human-Level Control through Deep Reinforcment Learning
%   - Apprentiship Reinformcent Learning 
%     - FYP7 - "An Application of Reinformcent Learning to Aerobatic Helicopter Flight (P. Abel 2007)" - done
%   - Demonstration
%     - FYP9 - DeepQ Learning from Demonstration
%     - FYP-16 - Forgetful Experience Replay in Hierechical Reinfrocment Learning from Expert Demonstrations.
%     - FYP17 - Mapless navigation for UAVs via Reinforcement Learning
%   - Transfer Learning
%     - FYP10 - Soft Actor-Critic with Inhibitory Networks for Retraining UAV Controllers Faster
%   - Learned Skills
%     - FYP-15 - Demonstration Guided Reinforcment Learning with Learned Skills

% Previous Work - Fabian

Previous work has explored the use of Reinforcement Learning in automatically generating trajectories for drone perching\cite{learnedTetheredPerchingFabian}.
In this approach a Soft Actor Critic algorithm was employed to develop a series of energy optimised trajectories.
The trajectory was divided into three seperate stages.
The initial approach and contact phases used trajectories formulated using analytical solutions.
The Reinformcent Learning aspect was specifically applied for the more complex manuver of flipping the drone beneath the branch.

\todo{diagram for this manuever}
A Markov Decision Process was defined using observations $o_{t} \in O$ where $o_{t}$ denotes the drone's relative position and $a_{t}$ determines its roll rate.
An experimental baseline provided a potential solution using a constant roll rate.
This method integreated two components of a reward function.
In the initial training stages, the algorithm priotitised conforming to the baseline.
As training progresses, there is a shift in the reward function's focus, increasingly favouring a faster, more energy efficient trajectory.
This shift allows a greater exploration of the parameter space.

\[
R_{1}(s_{t}, a_{t}) = 
\begin{cases} 
I(s_{t}, a_{t}) & \text{for } t \leq t_{I} \\
I(s_{t}, a_{t}) M(t) & \text{else} 
\end{cases}
\]

\[I(s_{t}, a_{t}) = 3 \times 10 ^ 5 \times (0.1 - min(|s_{\alpha} - s'_{\alpha}|, 0.1)) ^ 4\]
\[M(t) = \frac{t_{max} - t}{t_{max} - t_{I}}\]

\[
R_{2}(s_{t}, a_{t}) =
\begin{cases}
  R_{L} + \frac{\Delta X_{target} - 0.005}{l_{r}} \times 50 & \text{for } \frac{d_{target}}{d_{drone}} > 1 \\
  R_{L} + \frac{\Delta X_{branch}}{d_{drone}} \times 100 & \text{else}
\end{cases}
\]

\todo{rephrase this section}
where $s_{\alpha}$ and $s_{\alpha}$ are the roll angel of the current and the baseline state; 
$t_{max}$ is the maximum time of the simulation; 
$t_{I}$ is the time where the final point should be reacted.
where RL is the reward from the last step dtarget is the distance to the target position; 
ddrone is the size of the drone; 
∆Xtarget and ∆Xbranch is the change in the distance made to the final position and to the branch, and lr is the length of the rope.

However this method may constrain the range of possible solutions.
By favouring approaches that resemble the baseline, this could prevent the system from learning novel, potentially more efficient strategies.
Since just roll rate was explored, this limits the range of solutions into 2 dimensions which reduces the computational complexity but may not fully explore paths.
Additioanlly one of the main challenges in the previous work was simulation.
Accurately modelling the dynamics between the tether and the drone presented significant challenges.
Performing training in a real-world setting would be exceptionally difficult, given the extensive number of trials required and the risk of errors causing physical damage to the drone.

There is a need to devise a system capable of efficiently learning from a very limited number of experiments, while ensuring safety of the drone. \\\\

% FYP7 - "An Application of Reinformcent Learning to Aerobatic Helicopter Flight (P. Abel 2007)" - Demonstration
One of the earliest applications of Reinforcement Learning (RL) was in Aerobatic Helicopter Flight\cite{abbeelRLAerobaticFlight}.
Inverse Reinforcment Learning(IRL) was employed to learn a helicopter model and its associated reward function from human demonstrations.
This led to the first successfuly autonomous completion of four complex manuevers.
This apporach was motivated by the high amount of exploratory searching required in conventional Reinforcement Learning, which would lead to unsafe conditions and crashes.
Instead Apprentiship Learning was employed, allowing the system to create a model from demonstration flight data for each manuever.
This model was then trialled in simulation before finally being tried out on a physical helicopter.
The reward function consisted of 24 components, the balance between them was defined via an IRL algorithm.
However they found that strictly following the reward weights generated by this algorithm often led to unsafe conditions for the helicopter.
So, the reward weights were iteratively hand chosen from a mixture of the algorithm suggestions and inherent intuition. 
To address the risk of an unstable policy that was prone to fluctuating between extreme values, a term was added to the reward function, penalising change in inputs over successive time steps.

In the case of two manuevers, a single iteration of the process involving just 5 minutes of demonstration flight data proved sufficient to generate an accurate safe manuever.
For the remaining two manuevers, this process was repeated twice using further flight data in order to achieve a good result.
This study demonstraits that it is possible to effectively utilise demonstration data in order to massively speed up training of a reinforcement agent.
A key insight of this work was the minimisation of change in actions between subsequent steps to reduce the possibility of instability.
This particular work led to further exploration in the field of using demonstration data when trying to achieve a solution in areas where traditional simulation work was either too computationally expensive or difficult to achieve.

However a notable limitation of this approach is its reliance on expert generated demonstration data.
The demonstration data is presumed to represent the optimal solution, an assumption that is unlikely to hold true in the vast majority of applications.
The agent's performance is inherently limited by the expertise of the demonstrator.
Sicne the reward function is estabilished from the demonstration data, there lacks a mechanism to reward or even acknowlege improvements beyond the demonstrated capabilities.
This presents a challenge in scenerios where expert data is not necessarily possible or where surpassing the skill of a human pilot is required.

